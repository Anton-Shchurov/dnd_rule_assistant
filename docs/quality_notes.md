# Заметки по качеству RAG (Quality Notes)

## Текущие проблемы
1. **Релевантность поиска**: Часто находятся общие слова (например, "игрок", "мир"), но теряется конкретный контекст вопроса (например, "типы игроков" или "как создать свой мир").
2. **Обрывки контекста**: Чанки могут быть слишком короткими или вырванными из середины абзаца, теряя семантику.
3. **Пропуски**: Иногда даже при наличии информации в книге ретривер её не находит из-за неточного совпадения векторов (особенно для абстрактных вопросов).

## Гипотезы улучшения
1. **Увеличение initial_k**: С текущих k=5 (или 20 для rerank) поднять до k=50+ для реранкинга, чтобы захватить больше потенциально полезных, но "далеких" по вектору фрагментов.
2. **Singleton Reranker**: Чтобы не грузить модель на каждый запрос, сделать Reranker синглтоном (уже частично реализовано через кэширование sentence-transformers, но лучше явно).
3. **Промпт-инжиниринг**:
    - Явно указывать модели, что если точного ответа нет, но есть близкие концепции, можно аккуратно обобщить (но не выдумывать).
    - Добавить примеры (few-shot) в системный промпт для сложных случаев.
4. **Улучшение чанкинга (долгосрочно)**: Сейчас чанкинг по токенам. Возможно, стоит перейти на семантический чанкинг или иерархический (Parent Document Retriever).

---

## Инструкция по улучшению качества (Quality Loop)

Этот процесс (цикл) нужно повторять при каждом изменении промптов, параметров поиска или кода.

### Шаг 1. Запуск оценки (Baseline)
Сначала узнаем текущие метрики на эталонном наборе вопросов.

```bash
# Запускает прогон по 20 вопросам из dataset.jsonl
# Использует LLM для оценки качества ответа (0.0 - 1.0)
poetry run python scripts/run_eval.py
```

**Что смотреть в выводе консоли:**
- `Average Recall@5`: Как часто правильный кусок текста попадает в топ-5 найденных. Если < 0.7, проблема в поиске (Retriever).
- `Average F1` (LLM Score): Насколько ответ бота совпадает по смыслу с эталоном. Если < 0.8, проблема в генерации (LLM) или контексте.

### Шаг 2. Анализ отчёта
Скрипт создаёт файл отчёта в `logs/eval/eval_YYYYMMDD_....jsonl`. Откройте его (можно в VS Code или текстовом редакторе) и найдите строки с низким `f1` (например, 0.0 или 0.5).

Пример записи ошибки:
```json
{
  "question": "Какие бывают типы игроков?",
  "f1": 0.0,
  "reasoning": "Ответ системы утверждает, что информации нет, хотя эталон перечисляет 7 типов.",
  "retrieved_chunks": ["..."], 
  "expected_chunks": ["dmg_ch06_0011"]
}
```

**Вопросы для анализа:**
1. **Был ли найден нужный чанк?**
   - Сравните `retrieved_chunks` и `expected_chunks`.
   - Если нужного ID нет в списке -> Проблема в **Поиске** (Retriever).
   - Если нужный ID есть, но ответ плохой -> Проблема в **Генерации** (LLM/Prompt).

### Шаг 3. Внесение изменений
В зависимости от диагноза, меняем настройки.

**Если проблема в Поиске (низкий Recall):**
- Увеличить количество кандидатов для реранкинга (`initial_k`).
  - *Где*: `src/dnd_rag/core/pipelines.py` -> `answer_query_pipeline`.
- Изменить параметры чанкинга (размер чанка).
  - *Где*: `configs/ingest.yaml` (требует переиндексации: `cli chunks` -> `cli index`).
- Включить/заменить модель реранкинга.

**Если проблема в Генерации (низкий F1 при высоком Recall):**
- Улучшить системный промпт (дать инструкции, как отвечать).
  - *Где*: `configs/prompts.yaml`.
- Изменить температуру генерации.
  - *Где*: `src/dnd_rag/interfaces/cli.py` или конфиг.

### Шаг 4. Проверка гипотезы
Снова запускаем оценку:
```bash
poetry run python scripts/run_eval.py
```
Сравниваем новые `Average Recall` и `Average F1` с предыдущими.

### Полезные команды для отладки конкретного вопроса
Если вы работаете над одним сложным вопросом, не обязательно гонять весь тест. Используйте CLI с флагом отладки:

```bash
poetry run python -m dnd_rag.interfaces.cli ask "Ваш сложный вопрос" --debug-log
```
Это выведет таблицу с найденными чанками и их оценками (vector score / rerank score), что поможет понять, почему нужный документ не попал в топ.
