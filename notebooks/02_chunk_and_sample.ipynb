{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Нормализация, секцияция и чанкинг Markdown\n",
        "\n",
        "В этом ноутбуке разбиваем Markdown по заголовкам и создаём чанки (~800 токенов, overlap 120). Результат сохраняется в JSONL для последующей векторизации и загрузки в Qdrant.\n",
        "\n",
        "- Вход: `data/processed/md/{PHB,DMG}.md`\n",
        "- Выход: `data/processed/chunks/{PHB,DMG}.jsonl`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 1. Нормализация Markdown (очистка шума)\n",
        "\n",
        "Перед чанкингом очищаем Docling/OCR‑артефакты и приводим текст к «готовому к векторизации» виду. Используем объединённый набор правил:\n",
        "\n",
        "- **Декодирование спецпоследовательностей**: `&amp;` → `&`, `/uniXXXX` → соответствующий символ Юникода.\n",
        "- **Склейка разнесённых прописных**: `Г ЛАВА`, `Ч АСТЬ`, `D U N G E O N S` → цельные слова.\n",
        "- **Переносы и разрывы**: `буква-\\nбуква` → склейка; внутрисловной `\\n` → пробел.\n",
        "- **Дефис/тире и пробелы**: внутри слов `a - b` → `a-b`; между словами ` - ` → ` — `; лишние пробелы перед знаками препинания убираются.\n",
        "- **Омографы (латиница↔кириллица)**: латинские `A,B,C,E,H,K,M,O,P,T,X,Y` и строчные аналоги переводятся в кириллицу внутри русских слов.\n",
        "- **Частный артефакт `fа`**: удаляем начальное латинское `f` перед русской гласной (`fа` → `а`).\n",
        "- **Сдвоенные слова** после склейки строк: `персонажи персонажи` → `персонажи`.\n",
        "- **Unicode NFC** и финальная чистка пробелов/пустых строк.\n",
        "\n",
        "Результат пишем в `data/processed/md_clean/*.md` и используем далее для чанкинга.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/Shchurov/Хранилище/Документы/05_Программирование/02_projects/personal/dnd_rule_assistant/data/processed/md_clean/DMG.md'),\n",
              " WindowsPath('C:/Users/Shchurov/Хранилище/Документы/05_Программирование/02_projects/personal/dnd_rule_assistant/data/processed/md_clean/PHB.md')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Нормализация исходных Markdown в отдельную директорию\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "try:\n",
        "    # Сработает, если код запущен как скрипт, а не ноутбук\n",
        "    repo_root = Path(__file__).resolve().parent.parent\n",
        "except NameError:\n",
        "    # В ноутбуке __file__ нет — ищем корень по структуре проекта\n",
        "    repo_root = _detect_repo_root()\n",
        "\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "from dnd_rag.core.pipelines import normalize_md_dir_pipeline  # type: ignore\n",
        "\n",
        "RAW_MD_DIR = repo_root / 'data' / 'processed' / 'md'\n",
        "CLEAN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'\n",
        "\n",
        "normalized = normalize_md_dir_pipeline(RAW_MD_DIR, CLEAN_MD_DIR)\n",
        "normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 2. Чанкинг очищенного Markdown\n",
        "\n",
        "Берём файлы из `data/processed/md_clean`, секционируем по заголовкам и режем на перекрывающиеся чанки (~800 токенов, overlap 120). Выход — `data/processed/chunks/*.jsonl` для последующей векторизации/загрузки в Qdrant. Используем `chunk_docs_pipeline` с дефолтной конфигурацией.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт и запуск пайплайна чанкинга\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Добавляем src/ в PYTHONPATH\n",
        "repo_root = Path(__file__).resolve().parents[1]\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "from dnd_rag.core.pipelines import chunk_docs_pipeline #type: ignore\n",
        "from dnd_rag.core.config import DEFAULT_CONFIG_PATH #type: ignore\n",
        "\n",
        "IN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'\n",
        "OUT_CH_DIR = repo_root / 'data' / 'processed' / 'chunks'\n",
        "CONFIG = DEFAULT_CONFIG_PATH\n",
        "\n",
        "produced = chunk_docs_pipeline(IN_MD_DIR, OUT_CH_DIR, config_path=CONFIG)\n",
        "produced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Мини-поиск (sanity check)\n",
        "\n",
        "Выполним простой поиск по ключевым словам, чтобы убедиться, что в чанках есть релевантные фрагменты. Для производительного поиска позже будет использован Qdrant + эмбеддинги, здесь — лишь быстрая проверка.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Простой keyword-поиск по JSONL\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "repo_root = Path(__file__).resolve().parents[1]\n",
        "CH_DIR = repo_root / 'data' / 'processed' / 'chunks'\n",
        "\n",
        "files = sorted(CH_DIR.glob('*.jsonl'))\n",
        "rows = []\n",
        "for fp in files:\n",
        "    with fp.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "\n",
        "query = 'что такое преимущество'\n",
        "terms = [t for t in query.lower().split() if t]\n",
        "\n",
        "def score(text: str) -> int:\n",
        "    low = (text or '').lower()\n",
        "    return sum(low.count(t) for t in terms)\n",
        "\n",
        "rows.sort(key=lambda r: score(r.get('text', '')), reverse=True)\n",
        "rows[:5]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
