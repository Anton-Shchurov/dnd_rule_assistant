{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Нормализация, секцияция и чанкинг Markdown\n",
        "\n",
        "В этом ноутбуке разбиваем Markdown по заголовкам и создаём чанки (~800 токенов, overlap 120). Результат сохраняется в JSONL для последующей векторизации и загрузки в Qdrant.\n",
        "\n",
        "- Вход: `data/processed/md/{PHB,DMG}.md`\n",
        "- Выход: `data/processed/chunks/{PHB,DMG}.jsonl`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 1. Нормализация Markdown (очистка шума)\n",
        "\n",
        "Перед чанкингом очищаем Docling/OCR‑артефакты и приводим текст к «готовому к векторизации» виду. Используем объединённый набор правил:\n",
        "\n",
        "- **Декодирование спецпоследовательностей**: `&amp;` → `&`, `/uniXXXX` → соответствующий символ Юникода.\n",
        "- **Удаление шумных строк**: сбрасываем короткие OCR-артефакты вроде `@DMG.md (2-11)` или одиночные знаки без букв.\n",
        "- **Склейка разорванных слов и переносов**: `П оследующие` → `Последующие`, `D UNGEONS` → `DUNGEONS`, `буква-\\nбуква` → `буквабуква`, внутрисловной `\\n` → пробел, лишние пустые строки схлопываются.\n",
        "- **Дефис/тире и пробелы**: внутри слов `a - b` → `a-b`; между словами ` - ` → ` — `; лишние пробелы перед знаками препинания убираются.\n",
        "- **Восстановление буквиц**: при наличии `pymorphy2` восстанавливаем пропавшую первую букву в параграфах с буквицы.\n",
        "- **Омографы (латиница↔кириллица)**: латинские `A,B,C,E,H,K,M,O,P,T,X,Y` и строчные аналоги переводятся в кириллицу внутри русских слов.\n",
        "- **Частный артефакт `fа`**: удаляем начальное латинское `f` перед русской гласной (`fа` → `а`).\n",
        "- **Сдвоенные слова** после склейки строк: `персонажи персонажи` → `персонажи`.\n",
        "- **Unicode NFC** и финальная чистка пробелов/пустых строк.\n",
        "- **LLM-постобработка (опция)**: параметры находятся в `configs/ingest.yaml` (секция `llm_postprocess`). При `enabled: true` ищем подозрительные абзацы, отправляем их в OpenAI через LangChain и фиксируем изменения в `logs/llm_cleanup/*.json`. Ключ и модель кладём в `.env` (`OPENAI_API_KEY`, `OPENAI_MODEL`), файл в git не добавляем.\n",
        "\n",
        "Результат пишем в `data/processed/md_clean/*.md` и используем далее для чанкинга.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Нормализация исходных Markdown в отдельную директорию\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "try:\n",
        "    # Сработает, если код запущен как скрипт, а не ноутбук\n",
        "    repo_root = Path(__file__).resolve().parent.parent\n",
        "except NameError:\n",
        "    # В ноутбуке __file__ нет — ищем корень по структуре проекта\n",
        "    repo_root = _detect_repo_root()\n",
        "\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "from dnd_rag.core.pipelines import normalize_md_dir_pipeline  # type: ignore\n",
        "from dnd_rag.core.config import DEFAULT_CONFIG_PATH  # type: ignore\n",
        "\n",
        "RAW_MD_DIR = repo_root / 'data' / 'processed' / 'md'\n",
        "CLEAN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'\n",
        "\n",
        "normalized = normalize_md_dir_pipeline(\n",
        "    RAW_MD_DIR,\n",
        "    CLEAN_MD_DIR,\n",
        "    config_path=DEFAULT_CONFIG_PATH,\n",
        ")\n",
        "normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 2. LLM-постобработка (опционально)\n",
        "\n",
        "Этап запускается вручную после полной нормализации. Используются переменные окружения `LLM_API_KEY`, `MODEL`, при необходимости `URL_MODEL` (для OpenRouter совместимых провайдеров). Результат — инплейс‑правки в `data/processed/md_clean/*.md` и журнал `logs/llm_cleanup/*.json` (пары before/after).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Запуск LLM-постобработки (при необходимости)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "try:\n",
        "    repo_root\n",
        "except NameError:\n",
        "    repo_root = _detect_repo_root()\n",
        "\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "from dnd_rag.core.pipelines import llm_cleanup_md_dir_pipeline  # type: ignore\n",
        "from dnd_rag.core.config import DEFAULT_CONFIG_PATH  # type: ignore\n",
        "\n",
        "CLEAN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'\n",
        "\n",
        "# Опционально можно переопределить параметры\n",
        "produced = llm_cleanup_md_dir_pipeline(\n",
        "    CLEAN_MD_DIR,\n",
        "    config_path=DEFAULT_CONFIG_PATH,\n",
        "    # min_score=0.45,\n",
        "    # max_paragraphs=500,\n",
        ")\n",
        "produced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 3. Чанкинг очищенного Markdown\n",
        "\n",
        "Берём файлы из `data/processed/md_clean`, секционируем по заголовкам и режем на перекрывающиеся чанки (~800 токенов, overlap 120). Выход — `data/processed/chunks/*.jsonl` для последующей векторизации/загрузки в Qdrant. Используем `chunk_docs_pipeline` с дефолтной конфигурацией.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shchurov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymorphy2\\analyzer.py:114: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "2025-11-11 23:52:58,435 - INFO - Loading dictionaries from c:\\Users\\Shchurov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymorphy2_dicts_ru\\data\n",
            "2025-11-11 23:52:58,511 - INFO - format: 2.4, revision: 417127, updated: 2020-10-11T15:05:51.070345\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CFG] ingest.yaml: C:\\Users\\Shchurov\\Хранилище\\Документы\\05_Программирование\\02_projects\\personal\\dnd_rule_assistant\\configs\\ingest.yaml (exists=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/Shchurov/Хранилище/Документы/05_Программирование/02_projects/personal/dnd_rule_assistant/data/processed/chunks/DMG.jsonl')]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Импорт и запуск пайплайна чанкинга\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# Добавляем src/ в PYTHONPATH\n",
        "try:\n",
        "    repo_root\n",
        "except NameError:\n",
        "    try:\n",
        "        repo_root = Path(__file__).resolve().parents[1]\n",
        "    except NameError:\n",
        "        repo_root = _detect_repo_root()\n",
        "\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "from dnd_rag.core.pipelines import chunk_docs_pipeline #type: ignore\n",
        "from dnd_rag.core.config import DEFAULT_CONFIG_PATH #type: ignore\n",
        "\n",
        "IN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'\n",
        "OUT_CH_DIR = repo_root / 'data' / 'processed' / 'chunks'\n",
        "CONFIG = DEFAULT_CONFIG_PATH\n",
        "\n",
        "produced = chunk_docs_pipeline(IN_MD_DIR, OUT_CH_DIR, config_path=CONFIG)\n",
        "produced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Мини-поиск (sanity check)\n",
        "\n",
        "Выполним простой поиск по ключевым словам, чтобы убедиться, что в чанках есть релевантные фрагменты. Для производительного поиска позже будет использован Qdrant + эмбеддинги, здесь — лишь быстрая проверка.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Простой keyword-поиск по JSONL\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "try:\n",
        "    repo_root\n",
        "except NameError:\n",
        "    try:\n",
        "        repo_root = Path(__file__).resolve().parents[1]\n",
        "    except NameError:\n",
        "        repo_root = _detect_repo_root()\n",
        "\n",
        "CH_DIR = repo_root / 'data' / 'processed' / 'chunks'\n",
        "\n",
        "files = sorted(CH_DIR.glob('*.jsonl'))\n",
        "rows = []\n",
        "for fp in files:\n",
        "    with fp.open('r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "\n",
        "query = 'что такое преимущество'\n",
        "terms = [t for t in query.lower().split() if t]\n",
        "\n",
        "def score(text: str) -> int:\n",
        "    low = (text or '').lower()\n",
        "    return sum(low.count(t) for t in terms)\n",
        "\n",
        "rows.sort(key=lambda r: score(r.get('text', '')), reverse=True)\n",
        "rows[:5]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
