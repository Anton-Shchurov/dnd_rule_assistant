{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Нормализация, секционирование и чанкинг Markdown\n",
        "\n",
        "В этом ноутбуке выполняем три последовательных этапа обработки Markdown-документов для подготовки к векторизации и загрузке в Qdrant:\n",
        "\n",
        "## Этапы обработки\n",
        "\n",
        "1. **Глубокая нормализация** — очистка OCR-артефактов и приведение текста к корректному виду\n",
        "2. **Секционирование** — разбиение документа на логические разделы по заголовкам\n",
        "3. **Чанкинг** — деление разделов на перекрывающиеся фрагменты оптимального размера\n",
        "\n",
        "## Пути к файлам\n",
        "\n",
        "- **Вход (этап 1)**: `data/processed/md/{PHB,DMG}.md` — Markdown после Docling\n",
        "- **Промежуточный результат**: `data/processed/md_clean/{PHB,DMG}.md` — очищенный Markdown\n",
        "- **Выход (этап 3)**: `data/processed/chunks/{PHB,DMG}.jsonl` — чанки с метаданными в формате JSONL\n",
        "\n",
        "## Параметры чанкинга\n",
        "\n",
        "- **Размер чанка**: ~800 токенов (настраивается в `configs/ingest.yaml`)\n",
        "- **Перекрытие**: 120 токенов для сохранения контекста между чанками\n",
        "- **Метаданные**: каждый чанк содержит ID, название книги, главу, раздел, диапазон страниц\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 1. Нормализация Markdown (очистка шума)\n",
        "\n",
        "Перед чанкингом очищаем Docling/OCR‑артефакты и приводим текст к «готовому к векторизации» виду. Используется функция `deep_normalize_markdown`, которая последовательно применяет 13 этапов очистки:\n",
        "\n",
        "### Этапы нормализации (в порядке применения)\n",
        "\n",
        "**1. Унификация и декодирование**\n",
        "- Унификация переводов строк (Unix-стиль `\\n`)\n",
        "- Декодирование HTML-сущностей: `&amp;` → `&`, `&quot;` → `\"`\n",
        "- Декодирование Unicode escape-последовательностей: `/uniXXXX` → соответствующий символ\n",
        "\n",
        "**2. Базовая нормализация**\n",
        "- Исправление мягких переносов: `слово-\\nпродолжение` → `словопродолжение`\n",
        "- Удаление конечных пробелов\n",
        "- Схлопывание избыточных пустых строк (max 2 подряд)\n",
        "\n",
        "**3. Удаление шумных строк**\n",
        "- Сбрасываем OCR-артефакты: `@DMG.md (2-11)`, короткие строки без букв\n",
        "\n",
        "**4. Склейка разорванных слов**\n",
        "- `П оследующие` → `Последующие`\n",
        "- `D UNGEONS` → `DUNGEONS`\n",
        "- Внутрисловные переносы строк заменяются пробелом\n",
        "\n",
        "**5. Исправление дефисов и тире**\n",
        "- Внутри слов: `a - b` → `a-b`\n",
        "- Между словами: ` - ` → ` — ` (длинное тире)\n",
        "- Удаление лишних пробелов перед знаками препинания\n",
        "\n",
        "**6. Восстановление буквиц** (требуется `pymorphy2`)\n",
        "- Восстанавливаем пропавшую первую букву в параграфах, начинающихся с буквицы\n",
        "\n",
        "**7-8. Исправление омографов и гомоглифов**\n",
        "- Латинские буквы → кириллица внутри русских слов: `A,B,C,E,H,K,M,O,P,T,X,Y` + строчные\n",
        "- Дополнительные гомоглифы: `h→н`, `m→м`, `t→т` (только в словах с кириллицей)\n",
        "\n",
        "**9. Короткие латинские слова в русском тексте**\n",
        "- Безопасная замена: `Kak/Na/He/Ha/TOM/ACT/C` → `Как/На/Не/На/ТОМ/АСТ/С`\n",
        "- При наличии `pymorphy2` — проверка валидности по словарю\n",
        "\n",
        "**10. Замена цифр в русских словах**\n",
        "- `3/6/0` → `З/з`, `Б/б`, `О/о` (с учётом регистра и позиции)\n",
        "- `4` в конце слова → `й/Й`\n",
        "- **Игнорируются**: нотации костей (`1к4`, `к10`, `2d6`, `+N`)\n",
        "\n",
        "**11. Удаление артефакта 'f'**\n",
        "- Латинское `f` перед русской гласной: `fа` → `а`\n",
        "\n",
        "**12. Удаление сдвоенных слов**\n",
        "- После склейки строк: `персонажи персонажи` → `персонажи`\n",
        "\n",
        "**13. Финальная нормализация**\n",
        "- Unicode NFC (каноническая композиция)\n",
        "- Схлопывание множественных пробелов\n",
        "- Удаление конечных пробелов и лишних пустых строк\n",
        "\n",
        "### Результат\n",
        "\n",
        "Очищенные файлы сохраняются в `data/processed/md_clean/*.md` и используются для чанкинга на следующем этапе.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Нормализация исходных Markdown в отдельную директорию\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import difflib\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    \"\"\"\n",
        "    Ищет корень репозитория, поднимаясь по директориям вверх.\n",
        "    Корень определяется наличием папок 'src' и 'data'.\n",
        "    \"\"\"\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "\n",
        "try:\n",
        "    # Сработает, если код запущен как скрипт, а не ноутбук\n",
        "    repo_root = Path(__file__).resolve().parent.parent\n",
        "except NameError:\n",
        "    # В ноутбуке __file__ нет — ищем корень по структуре проекта\n",
        "    repo_root = _detect_repo_root()\n",
        "\n",
        "# Добавляем src/ в PYTHONPATH для импорта модулей проекта\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "# Импортируем функцию глубокой нормализации и путь к конфигурации\n",
        "from dnd_rag.core.pipelines import normalize_md_dir_pipeline  # type: ignore\n",
        "from dnd_rag.core.config import DEFAULT_CONFIG_PATH  # type: ignore\n",
        "\n",
        "# Указываем пути к директориям\n",
        "RAW_MD_DIR = repo_root / 'data' / 'processed' / 'md'  # Исходные Markdown после Docling\n",
        "CLEAN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'  # Очищенные Markdown\n",
        "\n",
        "# Запускаем глубокую нормализацию:\n",
        "# - Удаление OCR-артефактов и шумных строк\n",
        "# - Склейка разорванных слов и переносов\n",
        "# - Исправление омографов (латиница ↔ кириллица)\n",
        "# - Восстановление буквиц\n",
        "# - Замена цифр на буквы в русских словах\n",
        "# - Нормализация дефисов, тире и пробелов\n",
        "normalized = normalize_md_dir_pipeline(\n",
        "    RAW_MD_DIR,\n",
        "    CLEAN_MD_DIR,\n",
        "    config_path=DEFAULT_CONFIG_PATH,  # Используется для логирования, сама нормализация не зависит от конфига\n",
        ")\n",
        "\n",
        "# Логирование изменений в logs/ и подсчёт общей суммы изменений\n",
        "LOGS_DIR = repo_root / 'logs'\n",
        "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def _compute_changes_by_line_and_word(original_text: str, cleaned_text: str):\n",
        "    \"\"\"\n",
        "    Возвращает список изменений с привязкой к номеру строки исходного файла.\n",
        "    - Многострочные замены агрегируются как абзацы (before/after — блоки).\n",
        "    - Однострочные изменения детализируются по словам (before/after — фрагменты).\n",
        "    \"\"\"\n",
        "    # Унифицируем переводы строк и разбиваем на строки\n",
        "    orig_lines = original_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").split(\"\\n\")\n",
        "    clean_lines = cleaned_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").split(\"\\n\")\n",
        "\n",
        "    changes = []\n",
        "    sm = difflib.SequenceMatcher(a=orig_lines, b=clean_lines, autojunk=False)\n",
        "\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"equal\":\n",
        "            continue\n",
        "\n",
        "        # Если изменён блок из нескольких строк — считаем это абзацем\n",
        "        if (i2 - i1) > 1 or (j2 - j1) > 1:\n",
        "            before_block = \"\\n\".join(orig_lines[i1:i2])\n",
        "            after_block = \"\\n\".join(clean_lines[j1:j2])\n",
        "            line_no = i1 + 1  # 1‑based\n",
        "            changes.append({\n",
        "                \"line\": line_no,\n",
        "                \"before\": before_block,\n",
        "                \"after\": after_block,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Однострочные операции (replace/delete/insert) — детализация по словам\n",
        "        before_line = orig_lines[i1] if (i2 - i1) == 1 else \"\"\n",
        "        after_line = clean_lines[j1] if (j2 - j1) == 1 else \"\"\n",
        "\n",
        "        w1 = before_line.split()\n",
        "        w2 = after_line.split()\n",
        "        smw = difflib.SequenceMatcher(a=w1, b=w2, autojunk=False)\n",
        "        line_no = (i1 + 1) if (i2 - i1) >= 1 else max(i1, 0) + 1\n",
        "\n",
        "        for wtag, a1, a2, b1, b2 in smw.get_opcodes():\n",
        "            if wtag == \"equal\":\n",
        "                continue\n",
        "            before = \" \".join(w1[a1:a2]) if (a2 - a1) > 0 else \"\"\n",
        "            after = \" \".join(w2[b1:b2]) if (b2 - b1) > 0 else \"\"\n",
        "            changes.append({\n",
        "                \"line\": line_no,\n",
        "                \"before\": before,\n",
        "                \"after\": after,\n",
        "            })\n",
        "\n",
        "    return changes\n",
        "\n",
        "\n",
        "# Генерация логов по всем файлам, созданным пайплайном\n",
        "total_changes_all = 0\n",
        "for out_file in normalized:\n",
        "    out_path = Path(out_file)\n",
        "    in_path = RAW_MD_DIR / out_path.name\n",
        "    if not in_path.exists():\n",
        "        # Если исходного файла нет, пропускаем\n",
        "        continue\n",
        "\n",
        "    original_text = in_path.read_text(encoding=\"utf-8\")\n",
        "    cleaned_text = out_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    changes = _compute_changes_by_line_and_word(original_text, cleaned_text)\n",
        "    total_changes_all += len(changes)\n",
        "\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    book_code = out_path.stem  # Например, DMG\n",
        "    log_name = f\"{ts}_pdf-processing_{book_code}.json\"\n",
        "    log_path = LOGS_DIR / log_name\n",
        "\n",
        "    payload = {\n",
        "        \"file\": out_path.name,\n",
        "        \"original_path\": str(in_path),\n",
        "        \"clean_path\": str(out_path),\n",
        "        \"created_at\": ts,\n",
        "        \"total_changes\": len(changes),\n",
        "        \"changes\": changes,\n",
        "    }\n",
        "\n",
        "    log_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Сумма изменений: {total_changes_all}\")\n",
        "\n",
        "# Выводим список обработанных файлов\n",
        "normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Шаг 2. Секционирование очищенного Markdown\n",
        "\n",
        "На этом шаге мы строим структурированное представление книги без деления текста по длине:\n",
        "\n",
        "1. Проходим по очищенным файлам из `data/processed/md_clean`.\n",
        "2. Парсим заголовки (H1/H2/H3+) и сохраняем каждую секцию со связанной иерархией в `data/processed/sections/*.jsonl`.\n",
        "3. Для каждой секции фиксируем `book_title`, `chapter_title`, `section_path` и полный текст секции — именно эти данные пойдут в следующий этап чанкинга.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Shchurov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymorphy2\\analyzer.py:114: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "2025-11-16 22:52:08,626 - INFO - Loading dictionaries from c:\\Users\\Shchurov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymorphy2_dicts_ru\\data\n",
            "2025-11-16 22:52:08,711 - INFO - format: 2.4, revision: 417127, updated: 2020-10-11T15:05:51.070345\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[WindowsPath('C:/Users/Shchurov/Хранилище/Документы/05_Программирование/02_projects/personal/dnd_rule_assistant/data/processed/sections/DMG.jsonl')]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Импорт и запуск секционирования\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_repo_root() -> Path:\n",
        "    \"\"\"\n",
        "    Ищет корень репозитория, поднимаясь по директориям вверх.\n",
        "    Корень определяется наличием папок 'src' и 'data'.\n",
        "    \"\"\"\n",
        "    start = Path.cwd().resolve()\n",
        "    for p in [start] + list(start.parents):\n",
        "        if (p / 'src').is_dir() and (p / 'data').is_dir():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "# Проверяем, определена ли переменная repo_root из предыдущей ячейки\n",
        "try:\n",
        "    repo_root\n",
        "except NameError:\n",
        "    # Если переменная не определена, пытаемся найти корень репозитория\n",
        "    try:\n",
        "        repo_root = Path(__file__).resolve().parents[1]\n",
        "    except NameError:\n",
        "        repo_root = _detect_repo_root()\n",
        "\n",
        "# Добавляем src/ в PYTHONPATH для импорта модулей проекта\n",
        "sys.path.append(str(repo_root / 'src'))\n",
        "\n",
        "# Импортируем пайплайн секционирования\n",
        "from dnd_rag.core.pipelines import sections_from_md_pipeline  # type: ignore\n",
        "\n",
        "# Указываем пути к директориям\n",
        "IN_MD_DIR = repo_root / 'data' / 'processed' / 'md_clean'   # Очищенные Markdown\n",
        "OUT_SEC_DIR = repo_root / 'data' / 'processed' / 'sections'  # JSONL файлы с секциями\n",
        "\n",
        "# Запускаем секционирование (без деления по длине)\n",
        "produced = sections_from_md_pipeline(IN_MD_DIR, OUT_SEC_DIR)\n",
        "\n",
        "# Выводим список созданных JSONL файлов с секциями\n",
        "produced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Дальнейшие шаги\n",
        "\n",
        "Для индексации и эмбеддингов используйте отдельный ноутбук `03_index_and_embedding.ipynb`:\n",
        "- Шаг 1: инициализация/проверка коллекции в Qdrant (без запуска Docker — сервис уже работает)\n",
        "- Шаг 2: чанкинг из `data/processed/sections` и эмбеддинг текстов с загрузкой в Qdrant\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
